# entropy_nlp

An NLP pipeline for extracting medical test results from free text generated by OCR.

## Challenges

Documents are highly unstructured and erroneous.

Lack of any specifications for standardizing units.

Lack of any labelled/annotated training data.

Small amount of sample data.

Relevant-looking medical quantities may appear in a "Notes" section of the report, which are not actually relevant.

## Approach

First the text document is split into lines.  Then lines are classified by a fine-tuned language transformer as either containing a medical test result or not (this has limitations - see below). Date headers for the test results are extracted from the document (if they exist).  From the lines containing results, a "parameter" and (potentially multiple) "value" fields are extracted by a rules-based, fuzzy pattern matching approach.  Finally, we loop through the document using the date headers to extract the latest value for each parameter if more than one.  Duplicate parameters are omitted if their values are also the same (note: this implies parameter names are not necessarily unique in the output, but this could be changed).

The transformer is a distilBERT model fine-tuned on the sample data, manually annotated with `doccano` (ie, the lines containing test results are marked).  Since the appearance of test results seems to differ from *document to document*, it seemed wise to do the test/train split so that whole documents are in either side of the split.  In fact, a kind of 3-fold cross-validation was performed in this manner to estimate the generalization ability of the model (the average f1-score was 0.91).

The rules-based field extractor is quite complicated (and I think would be best replaced by a more robust statistical/NLP component): it first locates a possible "normal range" with regular expressions, then does a fuzzy search for the parameter name in the text preceding the range, then tries to find the numerical values.

## Limitations

Currently does not extract measurement units, only "parameter" and "value".

Only extracts results in "wide tabular" format, where one line contains the measurements over multiple dates.  However, there are also "long tabular" results, where each line corresponds to one date.

Date header extraction is based on the `dateparser` library, which aside from being very slow doesn't handle errors in the text.

## Potential Improvements

There are many!

The test result classifier could perhaps be trained on sentences instead of lines (this requires a good sentencizer for these documents).

Additionally, instead of binary text classification we could try token classification -- that way the model is able to use surrounding context to predict the test results.  Maybe one of these improvements is the way to make it work for long tabular results also.

The rules-based field extractor could be replaced by a more robust component.  If NER annotated training data were available (labeling parameter, values, units), then it would be an easier information extraction task and a LSTM-CRF model could possibly work.  Some preliminary investigations showed that LLMs are a promising approach to getting labelled data.